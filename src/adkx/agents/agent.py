# Copyright 2025 Wei Sun (Jack)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Simplified Agent implementation."""

from __future__ import annotations

from typing import AsyncGenerator

from google.adk.agents.invocation_context import InvocationContext
from google.adk.events.event import Event
from google.adk.models.google_llm import Gemini
from google.adk.models.llm_request import LlmRequest
from typing_extensions import override

from .base_agent import BaseAgent


class Agent(BaseAgent):
  """LLM-based agent.

  This is a simplified agent with just the essential fields:
  - model: The LLM to use
  - instruction: What the agent should do

  All fields are immutable after creation.

  Note: Tool execution will be redesigned with a new interface.
  """

  model: str
  """The model to use for this agent.

  Model name (e.g., "gemini-2.5-flash", "gemini-2.0-flash")
  """

  instruction: str = ''
  """Instructions for what this agent should do.

  Describes the agent's role, responsibilities, and behavior.
  """

  @override
  async def _run_async_impl(
      self, ctx: InvocationContext
  ) -> AsyncGenerator[Event, None]:
    """Core logic to run this agent via text-based conversation.

    Implements a simple LLM call:
    1. Build LlmRequest from session events
    2. Call LLM
    3. Yield model response event

    Args:
      ctx: InvocationContext, the invocation context for this agent.

    Yields:
      Event: the events generated by the agent.
    """
    # 1. Build LlmRequest from session events
    llm_request = LlmRequest(model=self.model)

    # Add system instruction if present
    if self.instruction:
      llm_request.append_instructions([self.instruction])

    # Add contents from session events
    for event in ctx.session.events:
      if event.content:
        llm_request.contents.append(event.content)

    # 2. Call LLM
    # TODO: Optimize by reusing Gemini client across invocations once we have
    # proper lifecycle management (e.g., server shutdown hooks to close clients)
    llm_response = None
    async for response in Gemini(model=self.model).generate_content_async(
        llm_request
    ):
      llm_response = response

    # 3. Construct Event from response and yield
    if llm_response is not None:
      model_event = Event(
          author=self.name,
          content=llm_response.content,
          invocation_id=ctx.invocation_id,
      )
      yield model_event

    # TODO: Add tool execution loop here
    # - Check if response contains function calls
    # - Execute tools and yield function response events
    # - Loop back to call LLM with tool results until no more function calls
